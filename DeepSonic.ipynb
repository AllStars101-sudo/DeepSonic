{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Sonic\n",
    "### ___Chris Pagolu, Joshua JJ Wonder___\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AllStars101-sudo/DeepSonic/blob/main/DeepSonic.ipynb)\n",
    "\n",
    "[DeepSonic](https://github.com/AllStars101-sudo/DeepSonic) is a fully open source Deep Learning music experiment that is capable of synthesizing, generating, remixing, modifying music, all using the power of AI. Thanks to powerful open-source libraries such as Magenta by Tensorflow/Google and Jukebox by OpenAI, we were able to create a multi-functional AI Audio Engineer. \n",
    "\n",
    "Note: This notebook runs all code natively. No cloud service is required unless you do not have a dedicated Nvidia GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Hardware Requirements and Recommendations\n",
    "\n",
    "The DeepSonic Experiment requires considerably powerful hardware. \n",
    "\n",
    "An NVIDIA Geforce RTX 2000 (Turing) Series GPU with 8GB of VRAM is required, at the least. A cloud-based NVIDIA Tesla or a server NVIDIA Quadro GPU with atleast 16GB VRAM is recommended, while a supercomputer will perform best, depending on the task. There are no explicit CPU requirements for DeepSonic, however, an AMD Ryzen 3 3200 or Intel Core i3 8100 (or higher) is recommended. The more powerful, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents:\n",
    "- 1. [How to get started?](#how-to-get-started)\n",
    "  - 1.1. [Quick Install Guide](#quick-install-guide)\n",
    "- 2. [DeepSynth](#deepsynth)\n",
    "- 3. [GANSynth](#gansynth)\n",
    "- 4. [DDSP Timbre Transfer](#ddsp-timbre-transfer)\n",
    "- 5. [Music Transformer](#music-transformer)\n",
    "  - 6.1. [Melody-conditioned Piano Transformer](#melody-conditioned-piano-transformer)\n",
    "- 6. [DeepLyrics](#deeplyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to get started?\n",
    "\n",
    "<a id=\"#how-to-get-started\">To</a> get started with DeepSonic, all you have to do is install Magenta and Jukebox from their official GitHub repositories.\n",
    "\n",
    "# Quick Install Guide:\n",
    "\n",
    "<a id=\"#quick-install-guide\">Run</a> the follow code in your shell (taken from the official Magenta and Jukebox wiki) to install the required tools. We recommend using a Debian-based operating system. The Windows Subsystem for Linux (WSL2) and Windows didn't appear to work at the time of our testing, possibly due to early support for Cuda on WSL. Also note: root privileges are required for installing audio libraries.\n",
    "\n",
    "```bash\n",
    "# Required commands:\n",
    "\n",
    "sudo apt-get update && sudo apt-get install build-essential libasound2-dev libjack-dev portaudio19-dev\n",
    "curl https://raw.githubusercontent.com/tensorflow/magenta/main/magenta/tools/magenta-install.sh > /tmp/magenta-install.sh\n",
    "bash /tmp/magenta-install.sh\n",
    "conda create --name jukebox python=3.7.5\n",
    "conda activate jukebox\n",
    "conda install mpi4py=3.0.3 # if this fails, try: pip install mpi4py==3.0.3\n",
    "conda install pytorch=1.4 torchvision=0.5 cudatoolkit=10.0 -c pytorch\n",
    "git clone https://github.com/openai/jukebox.git\n",
    "cd jukebox\n",
    "pip install -r requirements.txt\n",
    "pip install -e .\n",
    "conda install av=7.0.01 -c conda-forge \n",
    "pip install ./tensorboardX\n",
    "curl -o /path/to/dir/cs1-1pre.mid http://www.jsbach.net/midi/cs1-1pre.mid\n",
    "curl -o /path/to/dir/arp.mid http://storage.googleapis.com/magentadata/papers/gansynth/midi/arp.mid\n",
    "pip install -qU ddsp==1.6.5\n",
    "echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\n",
    "sudo apt-get install apt-transport-https ca-certificates gnupg\n",
    "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -\n",
    "sudo apt-get update && sudo apt-get install google-cloud-sdk\n",
    "gsutil -q -m cp -r gs://magentadata/models/music_transformer/primers/* /home/chris/Downloads/DeepSonic/\n",
    "gsutil -q -m cp gs://magentadata/soundfonts/Yamaha-C5-Salamander-JNv5.1.sf2 /home/chris/Downloads/DeepSonic/\n",
    "pip install -q 'tensorflow-datasets < 4.0.0'\n",
    "gsutil -q -m cp -r gs://magentadata/models/music_transformer/checkpoints/* /home/chris/Downloads/musictransformermodels/\n",
    "\n",
    " \n",
    "# Following two commands are optional: Apex for faster training with fused_adam \n",
    "\n",
    "conda install pytorch=1.1 torchvision=0.3 cudatoolkit=10.0 -c pytorch\n",
    "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Modules...\n",
      "\n",
      "WARNING:tensorflow:From /home/chris/miniconda3/envs/jukebox/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:549: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "Using cuda True\n",
      "Sucess!! Environment is now setup.\n"
     ]
    }
   ],
   "source": [
    "print('Importing Modules...\\n')\n",
    "#basic libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "%matplotlib inline\n",
    "\n",
    "#magenta libraries\n",
    "from magenta.models.nsynth import utils\n",
    "from magenta.models.nsynth.wavenet import fastgen\n",
    "from note_seq.notebook_utils import colab_play as play\n",
    "MIDI_SONG_DEFAULT = 'cs1-1pre.mid'\n",
    "MIDI_RIFF_DEFAULT = 'arp.mid'\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "import IPython\n",
    "import os\n",
    "import librosa\n",
    "from magenta.models.nsynth.utils import load_audio\n",
    "from magenta.models.gansynth.lib import flags as lib_flags\n",
    "from magenta.models.gansynth.lib import generate_util as gu\n",
    "from magenta.models.gansynth.lib import model as lib_model\n",
    "from magenta.models.gansynth.lib import util\n",
    "import matplotlib.pyplot as plt\n",
    "import note_seq\n",
    "from note_seq.notebook_utils import colab_play as play\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import crepe\n",
    "import ddsp\n",
    "import ddsp.training\n",
    "from ddsp.training.postprocessing import (\n",
    "    detect_notes, fit_quantile_transform\n",
    ")\n",
    "import gin\n",
    "import pickle\n",
    "from scipy.io import wavfile\n",
    "tf.disable_v2_behavior()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#music transformer libraries\n",
    "print('Copying Salamander piano SoundFont (via https://sites.google.com/site/soundfonts4u) from GCS...')\n",
    "!gsutil -q -m cp -r gs://magentadata/models/music_transformer/primers/* /home/chris/Downloads/DeepSonic/\n",
    "!gsutil -q -m cp gs://magentadata/soundfonts/Yamaha-C5-Salamander-JNv5.1.sf2 /home/chris/Downloads/DeepSonic/\n",
    "\n",
    "\n",
    "#jukebox libraries\n",
    "\n",
    "import jukebox\n",
    "import torch as t\n",
    "import librosa\n",
    "from jukebox.make_models import make_vqvae, make_prior, MODELS, make_model\n",
    "from jukebox.hparams import Hyperparams, setup_hparams\n",
    "from jukebox.sample import sample_single_window, _sample, \\\n",
    "                           sample_partial_window, upsample\n",
    "from jukebox.utils.dist_utils import setup_dist_from_mpi\n",
    "from jukebox.utils.torch_utils import empty_cache\n",
    "rank, local_rank, device = setup_dist_from_mpi()\n",
    "\n",
    "get_name = lambda f: os.path.splitext(os.path.basename(f))[0]\n",
    "\n",
    "print('Alrighty, we are done here.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug 22 15:07:26 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:09:00.0  On |                  N/A |\n",
      "| 25%   49C    P5    16W / 215W |   2234MiB /  7981MiB |      6%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1800      G   /usr/lib/xorg/Xorg               1217MiB |\n",
      "|    0   N/A  N/A      1933      G   /usr/bin/gnome-shell              199MiB |\n",
      "|    0   N/A  N/A      2524      G   ...AAAAAAAAA= --shared-files      233MiB |\n",
      "|    0   N/A  N/A      2659      G   ...AAAAAAAAA= --shared-files      201MiB |\n",
      "|    0   N/A  N/A     13622      C   ...3/envs/magenta/bin/python       97MiB |\n",
      "|    0   N/A  N/A     17164      G   telegram-desktop                  268MiB |\n",
      "|    0   N/A  N/A     82016      G   /usr/bin/totem                     10MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi #checks if cuda and nvidia drivers are working properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSynth\n",
    "### __Adapted from the [EZSynth Experiment](https://colab.research.google.com/notebooks/magenta/nsynth/nsynth.ipynb) by Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Douglas Eck, Karen Simonyan, Mohammad Norouzi__\n",
    "<a id=\"#deepsynth\">Neural</a> Audio Synthesis of Musical Notes with WaveNet Autoencoders\n",
    "\n",
    "### Additional Resources (as provided in the [EZSynth Notebook](https://colab.research.google.com/notebooks/magenta/nsynth/nsynth.ipynb)):\n",
    "* [Nat and Friends \"Behind the scenes\"](https://www.youtube.com/watch?v=BOoSy-Pg8is)\n",
    "* [Original Blog Post](https://magenta.tensorflow.org/nsynth)\n",
    "* [NSynth Instrument](https://magenta.tensorflow.org/nsynth-instrument)\n",
    "* [Jupyter Notebook Tutorial](https://magenta.tensorflow.org/nsynth-fastgen)\n",
    "* [ArXiv Paper](https://arxiv.org/abs/1704.01279)\n",
    "* [Github Code](https://github.com/tensorflow/magenta/tree/main/magenta/models/nsynth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two pretrained models to choose from (thanks, Google :)): one trained on the individual instrument notes of the [NSynth Dataset](https://magenta.tensorflow.org/datasets/nsynth) (\"Instruments\"), and another trained on a variety of voices in the wild for an art project (\"Voices\", mixture of singing and speaking). The Instruments model was trained on a larger quantity of data, so tends to generalize a bit better. Neither reconstructs audio perfectly, but both add their own unique character to sounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose a Model { vertical-output: true, run: \"auto\" }\n",
    "Model = \"Instruments\" #@param [\"Instruments\", \"Voices\"] {type:\"string\"}\n",
    "ckpts = {'Instruments': 'wavenet-ckpt/model.ckpt-200000',\n",
    "         'Voices': 'wavenet-voice-ckpt/model.ckpt-200000'}\n",
    "\n",
    "ckpt_path = ckpts[Model]\n",
    "print('Using model pretrained on %s.' % Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use local audio files\n",
    "\n",
    "In the next section, you may choose to specify which audio file you want to use for the audio synthesis. Note: the larger your audio file, the longer it'll take to encode and the longer it'll take to synthesize the audio, depending on how powerful your GPU is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Sound Length (in Seconds) { vertical-output: true, run: \"auto\" }\n",
    "Length = 60.0 #set the length of your synthesized audio\n",
    "SR = 16000\n",
    "SAMPLE_LENGTH = int(SR * Length) #audio length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload sound files (.wav, .mp3)\n",
    "\n",
    "try:\n",
    "  file_list, audio_list = [], [] #creates numpy arrays for file and audio lists\n",
    "  fname=\"test.mp3\" # name of the audio file\n",
    "  audio = utils.load_audio(fname, sample_length=SAMPLE_LENGTH, sr=SR)  #loads audio file for magenta to process\n",
    "  file_list.append(fname)\n",
    "  audio_list.append(audio)\n",
    "  names = [get_name(f) for f in file_list]\n",
    "  # Pad and peak normalize\n",
    "  for i in range(len(audio_list)):\n",
    "    audio_list[i] = audio_list[i] / np.abs(audio_list[i]).max()\n",
    "\n",
    "    if len(audio_list[i]) < SAMPLE_LENGTH:\n",
    "      padding = SAMPLE_LENGTH - len(audio_list[i])\n",
    "      audio_list[i] = np.pad(audio_list[i], (0, padding), 'constant')\n",
    "\n",
    "  audio_list = np.array(audio_list)\n",
    "except Exception as e:\n",
    "  print(\"Error encountered. Sure the file is .wav or .mp3? Does your GPU have enough memory left?\")\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code may take some time, depending on your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Encodings\n",
    "audio = np.array(audio_list)\n",
    "z = fastgen.encode(audio, ckpt_path, SAMPLE_LENGTH)\n",
    "print('Encoded %d files' % z.shape[0])\n",
    "\n",
    "\n",
    "# Start with reconstructions\n",
    "z_list = [z_ for z_ in z]\n",
    "name_list = ['recon_' + name_ for name_ in names]\n",
    "\n",
    "# Add all the mean interpolations\n",
    "n = len(names)\n",
    "for i in range(n - 1):\n",
    "  for j in range(i + 1, n):\n",
    "    new_z = (z[i] + z[j]) / 2.0\n",
    "    new_name = 'interp_' + names[i] + '_X_'+ names[j]\n",
    "    z_list.append(new_z)\n",
    "    name_list.append(new_name)\n",
    "\n",
    "print(\"%d total: %d reconstructions and %d interpolations\" % (len(name_list), n, len(name_list) - n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Step: Synthesize\n",
    "\n",
    "With a GPU, this should take about 4 minutes per 1 second of audio per a batch. Approximate time required for a 60 second song (~1,000,000 interpolations): 8-12 hours on a GeForce GPU and 2-8 hours on a Quadro or Tesla GPU. After that, your synthesized audio will appear in the same directory as this notebook (can be found as `recon_<name of audio>.mp3` or `recon_<name of audio>.wav`). )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Synthesize Interpolations\n",
    "print('Total Iterations to Complete: %d\\n' % SAMPLE_LENGTH)\n",
    "\n",
    "encodings = np.array(z_list)\n",
    "save_paths = [name + '.wav' for name in name_list]\n",
    "fastgen.synthesize(encodings,\n",
    "                   save_paths=save_paths,\n",
    "                   checkpoint_path=ckpt_path,\n",
    "                   samples_per_save=int(SAMPLE_LENGTH / 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANSynth\n",
    "### __Adapted from the [GANSynth Demo Notebook](https://colab.research.google.com/notebooks/magenta/nsynth/nsynth.ipynb) by the Magenta team.__\n",
    "\n",
    "<a id=\"#gansynth\">GANSynth</a> generates audio using Generative Adversarial Networks. GANSynth learns to produce individual instrument notes using the NSynth Dataset from Google. With pitch provided as a conditional attribute, the generator learns to use its latent space to represent different instrument timbres. This allows us to synthesize performances from MIDI files, either keeping the timbre constant, or interpolating between instruments over time. Rather than generate audio sequentially, GANSynth generates an entire sequence in parallel, synthesizing audio significantly faster than real-time on a modern GPU and ~50,000 times faster than a standard WaveNet. Unlike the WaveNet autoencoders from the original paper that used a time-distributed latent code, GANSynth generates the entire audio clip from a single latent vector, allowing for easier disentanglement of global features such as pitch and timbre. Using the NSynth dataset of musical instrument notes, we can independently control pitch and timbre.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/magentadata/papers/gansynth/figures/models.jpeg\" alt=\"GANSynth figure\" width=\"600\">\n",
    "\n",
    "\n",
    "### Additional Resources (as provided in the [GANSynth Notebook](https://colab.research.google.com/notebooks/magenta/gansynth/gansynth_demo.ipynb)):\n",
    "* [GANSynth ICLR paper](https://arxiv.org/abs/1809.11096)\n",
    "* [Audio Examples](http://goo.gl/magenta/gansynth-examples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "CKPT_DIR = '/home/chris/Downloads/all_instruments' #Load Checkpoint of model\n",
    "output_dir = '/home/chris/Downloads/DeepSonic/samples/gansynth' #where you want your final audio file to be saved\n",
    "BATCH_SIZE = 16\n",
    "SR = 16000\n",
    "\n",
    "# Load the model\n",
    "tf.reset_default_graph()\n",
    "flags = lib_flags.Flags({\n",
    "    'batch_size_schedule': [BATCH_SIZE],\n",
    "    'tfds_data_dir': 'gs://tfds-data/datasets',\n",
    "})\n",
    "model = lib_model.Model.load_from_path(CKPT_DIR, flags)\n",
    "\n",
    "# Helper functions\n",
    "def load_midi(midi_path, min_pitch=36, max_pitch=84):\n",
    "  \"\"\"Load midi as a notesequence.\"\"\"\n",
    "  midi_path = util.expand_path(midi_path)\n",
    "  ns = note_seq.midi_file_to_sequence_proto(midi_path)\n",
    "  pitches = np.array([n.pitch for n in ns.notes])\n",
    "  velocities = np.array([n.velocity for n in ns.notes])\n",
    "  start_times = np.array([n.start_time for n in ns.notes])\n",
    "  end_times = np.array([n.end_time for n in ns.notes])\n",
    "  valid = np.logical_and(pitches >= min_pitch, pitches <= max_pitch)\n",
    "  notes = {'pitches': pitches[valid],\n",
    "           'velocities': velocities[valid],\n",
    "           'start_times': start_times[valid],\n",
    "           'end_times': end_times[valid]}\n",
    "  return ns, notes\n",
    "\n",
    "def get_envelope(t_note_length, t_attack=0.010, t_release=0.3, sr=16000):\n",
    "  \"\"\"Create an attack sustain release amplitude envelope.\"\"\"\n",
    "  t_note_length = min(t_note_length, 3.0)\n",
    "  i_attack = int(sr * t_attack)\n",
    "  i_sustain = int(sr * t_note_length)\n",
    "  i_release = int(sr * t_release)\n",
    "  i_tot = i_sustain + i_release  # attack envelope doesn't add to sound length\n",
    "  envelope = np.ones(i_tot)\n",
    "  # Linear attack\n",
    "  envelope[:i_attack] = np.linspace(0.0, 1.0, i_attack)\n",
    "  # Linear release\n",
    "  envelope[i_sustain:i_tot] = np.linspace(1.0, 0.0, i_release)\n",
    "  return envelope\n",
    "\n",
    "def combine_notes(audio_notes, start_times, end_times, velocities, sr=16000):\n",
    "  \"\"\"Combine audio from multiple notes into a single audio clip.\n",
    "\n",
    "  Args:\n",
    "    audio_notes: Array of audio [n_notes, audio_samples].\n",
    "    start_times: Array of note starts in seconds [n_notes].\n",
    "    end_times: Array of note ends in seconds [n_notes].\n",
    "    sr: Integer, sample rate.\n",
    "\n",
    "  Returns:\n",
    "    audio_clip: Array of combined audio clip [audio_samples]\n",
    "  \"\"\"\n",
    "  n_notes = len(audio_notes)\n",
    "  clip_length = end_times.max() + 3.0\n",
    "  audio_clip = np.zeros(int(clip_length) * sr)\n",
    "\n",
    "  for t_start, t_end, vel, i in zip(start_times, end_times, velocities, range(n_notes)):\n",
    "    # Generate an amplitude envelope\n",
    "    t_note_length = t_end - t_start\n",
    "    envelope = get_envelope(t_note_length)\n",
    "    length = len(envelope)\n",
    "    audio_note = audio_notes[i, :length] * envelope\n",
    "    # Normalize\n",
    "    audio_note /= audio_note.max()\n",
    "    audio_note *= (vel / 127.0)\n",
    "    # Add to clip buffer\n",
    "    clip_start = int(t_start * sr)\n",
    "    clip_end = clip_start + length\n",
    "    audio_clip[clip_start:clip_end] += audio_note\n",
    "\n",
    "  # Normalize\n",
    "  audio_clip /= audio_clip.max()\n",
    "  audio_clip /= 2.0\n",
    "  return audio_clip\n",
    "\n",
    "# Plotting tools\n",
    "def specplot(audio_clip):\n",
    "  p_min = np.min(36)\n",
    "  p_max = np.max(84)\n",
    "  f_min = librosa.midi_to_hz(p_min)\n",
    "  f_max = 2 * librosa.midi_to_hz(p_max)\n",
    "  octaves = int(np.ceil(np.log2(f_max) - np.log2(f_min)))\n",
    "  bins_per_octave = 36\n",
    "  n_bins = int(bins_per_octave * octaves)\n",
    "  C = librosa.cqt(audio_clip, sr=SR, hop_length=2048, fmin=f_min, n_bins=n_bins, bins_per_octave=bins_per_octave)\n",
    "  power = 10 * np.log10(np.abs(C)**2 + 1e-6)\n",
    "  plt.matshow(power[::-1, 2:-2], aspect='auto', cmap=plt.cm.magma)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "\n",
    "print('And...... Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Interpolation\n",
    "\n",
    "These cells take the MIDI for a full song and interpolate between several random latent vectors (equally spaced in time) over the whole song. The result sounds like instruments that slowly and smoothly morph between each other.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_file = \"Bach Prelude (Default)\" #name of a default midi file, provided by Google\n",
    "\n",
    "midi_path = MIDI_SONG_DEFAULT\n",
    "\n",
    "ns, notes = load_midi(midi_path)\n",
    "print('Loaded {}'.format(midi_path))\n",
    "note_seq.plot_sequence(ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Random Interpolation\n",
    "Assign the number of seconds to take in interpolating between each random instrument. Larger numbers will have slower and smoother interpolations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds_per_instrument = 5 \n",
    "\n",
    "# Distribute latent vectors linearly in time\n",
    "z_instruments, t_instruments = gu.get_random_instruments(\n",
    "    model, notes['end_times'][-1], secs_per_instrument=seconds_per_instrument)\n",
    "\n",
    "# Get latent vectors for each note\n",
    "z_notes = gu.get_z_notes(notes['start_times'], z_instruments, t_instruments)\n",
    "\n",
    "# Generate audio for each note\n",
    "print('Generating {} samples...'.format(len(z_notes)))\n",
    "audio_notes = model.generate_samples_from_z(z_notes, notes['pitches'])\n",
    "\n",
    "# Make a single audio clip\n",
    "audio_clip = combine_notes(audio_notes,\n",
    "                           notes['start_times'],\n",
    "                           notes['end_times'],\n",
    "                           notes['velocities'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Synthesized Audio\n",
    "A [Constant-Q Spectogram](https://en.wikipedia.org/wiki/Constant-Q_transform) will also be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the audio\n",
    "print('\\nAudio:')\n",
    "IPython.display.Audio(audio_clip, rate=SR)\n",
    "print('CQT Spectrogram:')\n",
    "specplot(audio_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(output_dir, 'generated_clip.wav') #enter desired file name\n",
    "gu.save_wav(audio_clip, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate your own interpolation (custom interpolation)\n",
    "\n",
    "These cells allow you to choose two latent vectors and interpolate between them over a MIDI clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_file = \"Arpeggio (Default)\"\n",
    "\n",
    "midi_path = MIDI_RIFF_DEFAULT\n",
    "\n",
    "\n",
    "print('Loaded {}'.format(midi_path))\n",
    "note_seq.plot_sequence(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample some random instruments\n",
    "\n",
    "number_of_random_instruments = 10 #enter desired number of random instruments (max: 16)\n",
    "pitch_preview = 60\n",
    "n_preview = number_of_random_instruments\n",
    "\n",
    "pitches_preview = [pitch_preview] * n_preview\n",
    "z_preview = model.generate_z(n_preview)\n",
    "\n",
    "audio_notes = model.generate_samples_from_z(z_preview, pitches_preview)\n",
    "for i, audio_note in enumerate(audio_notes):\n",
    "  print(\"Instrument: {}\".format(i))\n",
    "  play(audio_note, sample_rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate custom interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruments = [0, 2, 4, 0]\n",
    "times = [0, 0.3, 0.6, 1.0]\n",
    "times[0] = -0.001\n",
    "times[-1] = 1.0\n",
    "\n",
    "z_instruments = np.array([z_preview[i] for i in instruments])\n",
    "t_instruments = np.array([notes_2['end_times'][-1] * t for t in times])\n",
    "\n",
    "# Get latent vectors for each note\n",
    "z_notes = gu.get_z_notes(notes_2['start_times'], z_instruments, t_instruments)\n",
    "\n",
    "# Generate audio for each note\n",
    "print('Generating {} samples...'.format(len(z_notes)))\n",
    "audio_notes = model.generate_samples_from_z(z_notes, notes_2['pitches'])\n",
    "\n",
    "# Make a single audio clip\n",
    "audio_clip = combine_notes(audio_notes,\n",
    "                           notes_2['start_times'],\n",
    "                           notes_2['end_times'],\n",
    "                           notes_2['velocities'])\n",
    "\n",
    "# Play the audio\n",
    "print('\\nAudio:')\n",
    "IPython.display.Audio(audio_clip, rate=SR)\n",
    "print('CQT Spectrogram:')\n",
    "specplot(audio_clip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the file\n",
    "fname = os.path.join(output_dir, 'generated_clip.wav') # name of the file\n",
    "gu.save_wav(audio_clip, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDSP Timbre Transfer\n",
    "### __Adapted from the [DDSP Timbre Transfer Demo](hhttps://colab.research.google.com/github/magenta/ddsp/blob/main/ddsp/colab/demos/timbre_transfer.ipynb#scrollTo=Go36QW9AS_CD) by the Magenta team.__\n",
    "\n",
    "<a id=\"#ddsp-timbre-transfer\">Convert</a> audio between sound sources with pretrained models. For example, you can try turning your voice into a violin, or scratching your laptop and seeing how it sounds as a flute!\n",
    "This section is a demo of timbre transfer using DDSP (Differentiable Digital Signal Processing). The model here is trained to generate audio conditioned on a time series of fundamental frequency and loudness.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/ddsp/additive_diagram/ddsp_autoencoder.png\" alt=\"DDSP process depiction\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplerate, audio = wavfile.read('generated_clip.wav')\n",
    "if len(audio.shape) == 1:\n",
    "  audio = audio[np.newaxis, :]\n",
    "print('\\nExtracting audio features...')\n",
    "\n",
    "\n",
    "# Setup the session.\n",
    "ddsp.spectral_ops.reset_crepe()\n",
    "\n",
    "# Compute features.\n",
    "start_time = time.time()\n",
    "audio_features = ddsp.training.metrics.compute_audio_features(audio)\n",
    "audio_features['loudness_db'] = audio_features['loudness_db'].astype(np.float32)\n",
    "audio_features_mod = None\n",
    "print('Audio features took %.1f seconds' % (time.time() - start_time))\n",
    "\n",
    "\n",
    "TRIM = -15\n",
    "# Plot Features.\n",
    "fig, ax = plt.subplots(nrows=3, \n",
    "                       ncols=1, \n",
    "                       sharex=True,\n",
    "                       figsize=(6, 8))\n",
    "ax[0].plot(audio_features['loudness_db'][:TRIM])\n",
    "ax[0].set_ylabel('loudness_db')\n",
    "\n",
    "ax[1].plot(librosa.hz_to_midi(audio_features['f0_hz'][:TRIM]))\n",
    "ax[1].set_ylabel('f0 [midi]')\n",
    "\n",
    "ax[2].plot(audio_features['f0_confidence'][:TRIM])\n",
    "ax[2].set_ylabel('f0 confidence')\n",
    "_ = ax[2].set_xlabel('Time step [frame]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'Violin' #@param ['Violin', 'Flute', 'Flute2', 'Trumpet', 'Tenor_Saxophone', 'Upload your own (checkpoint folder as .zip)']\n",
    "MODEL = model\n",
    "\n",
    "if model in {'Violin', 'Flute', 'Flute2', 'Trumpet', 'Tenor_Saxophone'}:\n",
    "  # Pretrained models.\n",
    "  PRETRAINED_DIR = '/home/chris/Downloads/timbre_models'\n",
    "  model_dir = PRETRAINED_DIR\n",
    "  gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
    "\n",
    "\n",
    "# Load the dataset statistics.\n",
    "DATASET_STATS = None\n",
    "dataset_stats_file = os.path.join(model_dir, 'dataset_statistics.pkl')\n",
    "print(f'Loading dataset statistics from {dataset_stats_file}')\n",
    "try:\n",
    "  if tf.io.gfile.exists(dataset_stats_file):\n",
    "    with tf.io.gfile.GFile(dataset_stats_file, 'rb') as f:\n",
    "      DATASET_STATS = pickle.load(f)\n",
    "except Exception as err:\n",
    "  print('Loading dataset statistics from pickle failed: {}.'.format(err))\n",
    "\n",
    "\n",
    "# Parse gin config,\n",
    "with gin.unlock_config():\n",
    "  gin.parse_config_file(gin_file, skip_unknown=True)\n",
    "\n",
    "# Assumes only one checkpoint in the folder, 'ckpt-[iter]`.\n",
    "ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]\n",
    "ckpt_name = ckpt_files[0].split('.')[0]\n",
    "ckpt = os.path.join(model_dir, ckpt_name)\n",
    "\n",
    "# Ensure dimensions and sampling rates are equal\n",
    "time_steps_train = gin.query_parameter('F0LoudnessPreprocessor.time_steps')\n",
    "n_samples_train = gin.query_parameter('Harmonic.n_samples')\n",
    "hop_size = int(n_samples_train / time_steps_train)\n",
    "\n",
    "time_steps = int(audio.shape[1] / hop_size)\n",
    "n_samples = time_steps * hop_size\n",
    "\n",
    "# print(\"===Trained model===\")\n",
    "# print(\"Time Steps\", time_steps_train)\n",
    "# print(\"Samples\", n_samples_train)\n",
    "# print(\"Hop Size\", hop_size)\n",
    "# print(\"\\n===Resynthesis===\")\n",
    "# print(\"Time Steps\", time_steps)\n",
    "# print(\"Samples\", n_samples)\n",
    "# print('')\n",
    "\n",
    "gin_params = [\n",
    "    'Harmonic.n_samples = {}'.format(n_samples),\n",
    "    'FilteredNoise.n_samples = {}'.format(n_samples),\n",
    "    'F0LoudnessPreprocessor.time_steps = {}'.format(time_steps),\n",
    "    'oscillator_bank.use_angular_cumsum = True',  # Avoids cumsum accumulation errors.\n",
    "]\n",
    "\n",
    "with gin.unlock_config():\n",
    "  gin.parse_config(gin_params)\n",
    "\n",
    "\n",
    "# Trim all input vectors to correct lengths \n",
    "for key in ['f0_hz', 'f0_confidence', 'loudness_db']:\n",
    "  audio_features[key] = audio_features[key][:time_steps]\n",
    "audio_features['audio'] = audio_features['audio'][:, :n_samples]\n",
    "\n",
    "\n",
    "# Set up the model just to predict audio given new conditioning\n",
    "model = ddsp.training.models.Autoencoder()\n",
    "model.restore(ckpt)\n",
    "\n",
    "# Build model by running a batch through it.\n",
    "start_time = time.time()\n",
    "_ = model(audio_features, training=False)\n",
    "print('Restoring model took %.1f seconds' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify conditioning\n",
    "These models were not explicitly trained to perform timbre transfer, so they may sound unnatural if the incoming loudness and frequencies are very different then the training data (which will always be somewhat true)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note Detection\n",
    "You can leave this at 1.0 for most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADJUST = True #change this to false if you want to manually adjust the pitch and loudness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiet parts without notes detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiet = 20 # max= 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Force pitch to nearest note (amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autotune = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_shift =  0 #shifts the pitch, max = 2, min = -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features_mod = {k: v.copy() for k, v in audio_features.items()}\n",
    "## Helper functions.\n",
    "def shift_ld(audio_features, ld_shift=0.0):\n",
    "  \"\"\"Shift loudness by a number of ocatves.\"\"\"\n",
    "  audio_features['loudness_db'] += ld_shift\n",
    "  return audio_features\n",
    "\n",
    "\n",
    "def shift_f0(audio_features, pitch_shift=0.0):\n",
    "  \"\"\"Shift f0 by a number of ocatves.\"\"\"\n",
    "  audio_features['f0_hz'] *= 2.0 ** (pitch_shift)\n",
    "  audio_features['f0_hz'] = np.clip(audio_features['f0_hz'], \n",
    "                                    0.0, \n",
    "                                    librosa.midi_to_hz(110.0))\n",
    "  return audio_features\n",
    "\n",
    "\n",
    "mask_on = None\n",
    "\n",
    "if ADJUST and DATASET_STATS is not None:\n",
    "  # Detect sections that are \"on\".\n",
    "  mask_on, note_on_value = detect_notes(audio_features['loudness_db'],\n",
    "                                        audio_features['f0_confidence'],\n",
    "                                        threshold)\n",
    "\n",
    "  if np.any(mask_on):\n",
    "    # Shift the pitch register.\n",
    "    target_mean_pitch = DATASET_STATS['mean_pitch']\n",
    "    pitch = ddsp.core.hz_to_midi(audio_features['f0_hz'])\n",
    "    mean_pitch = np.mean(pitch[mask_on])\n",
    "    p_diff = target_mean_pitch - mean_pitch\n",
    "    p_diff_octave = p_diff / 12.0\n",
    "    round_fn = np.floor if p_diff_octave > 1.5 else np.ceil\n",
    "    p_diff_octave = round_fn(p_diff_octave)\n",
    "    audio_features_mod = shift_f0(audio_features_mod, p_diff_octave)\n",
    "\n",
    "\n",
    "    # Quantile shift the note_on parts.\n",
    "    _, loudness_norm = fit_quantile_transform(\n",
    "        audio_features['loudness_db'],\n",
    "        mask_on,\n",
    "        inv_quantile=DATASET_STATS['quantile_transform'])\n",
    "\n",
    "    # Turn down the note_off parts.\n",
    "    mask_off = np.logical_not(mask_on)\n",
    "    loudness_norm[mask_off] -=  quiet * (1.0 - note_on_value[mask_off][:, np.newaxis])\n",
    "    loudness_norm = np.reshape(loudness_norm, audio_features['loudness_db'].shape)\n",
    "    \n",
    "    audio_features_mod['loudness_db'] = loudness_norm \n",
    "\n",
    "    # Auto-tune.\n",
    "    if autotune:\n",
    "      f0_midi = np.array(ddsp.core.hz_to_midi(audio_features_mod['f0_hz']))\n",
    "      tuning_factor = get_tuning_factor(f0_midi, audio_features_mod['f0_confidence'], mask_on)\n",
    "      f0_midi_at = auto_tune(f0_midi, tuning_factor, mask_on, amount=autotune)\n",
    "      audio_features_mod['f0_hz'] = ddsp.core.midi_to_hz(f0_midi_at)\n",
    "\n",
    "  else:\n",
    "    print('\\nSkipping auto-adjust (no notes detected or ADJUST box empty).')\n",
    "\n",
    "else:\n",
    "  print('\\nSkipping auto-adujst (box not checked or no dataset statistics found).')\n",
    "\n",
    "# Manual Shifts.\n",
    "audio_features_mod = shift_ld(audio_features_mod, loudness_shift)\n",
    "audio_features_mod = shift_f0(audio_features_mod, pitch_shift)\n",
    "\n",
    "\n",
    "\n",
    "# Plot Features.\n",
    "has_mask = int(mask_on is not None)\n",
    "n_plots = 3 if has_mask else 2 \n",
    "fig, axes = plt.subplots(nrows=n_plots, \n",
    "                      ncols=1, \n",
    "                      sharex=True,\n",
    "                      figsize=(2*n_plots, 8))\n",
    "\n",
    "if has_mask:\n",
    "  ax = axes[0]\n",
    "  ax.plot(np.ones_like(mask_on[:TRIM]) * threshold, 'k:')\n",
    "  ax.plot(note_on_value[:TRIM])\n",
    "  ax.plot(mask_on[:TRIM])\n",
    "  ax.set_ylabel('Note-on Mask')\n",
    "  ax.set_xlabel('Time step [frame]')\n",
    "  ax.legend(['Threshold', 'Likelihood','Mask'])\n",
    "\n",
    "ax = axes[0 + has_mask]\n",
    "ax.plot(audio_features['loudness_db'][:TRIM])\n",
    "ax.plot(audio_features_mod['loudness_db'][:TRIM])\n",
    "ax.set_ylabel('loudness_db')\n",
    "ax.legend(['Original','Adjusted'])\n",
    "\n",
    "ax = axes[1 + has_mask]\n",
    "ax.plot(librosa.hz_to_midi(audio_features['f0_hz'][:TRIM]))\n",
    "ax.plot(librosa.hz_to_midi(audio_features_mod['f0_hz'][:TRIM]))\n",
    "ax.set_ylabel('f0 [midi]')\n",
    "_ = ax.legend(['Original','Adjusted'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, resynthesize audio.\n",
    "Runs a batch of predictions and then plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af = audio_features if audio_features_mod is None else audio_features_mod\n",
    "\n",
    "# Run a batch of predictions.\n",
    "start_time = time.time()\n",
    "outputs = model(af, training=False)\n",
    "audio_gen = model.get_audio_from_outputs(outputs)\n",
    "print('Prediction took %.1f seconds' % (time.time() - start_time))\n",
    "\n",
    "# Plot\n",
    "print('Original')\n",
    "sr=44000\n",
    "IPython.display.Audio(audio, rate=sr)\n",
    "\n",
    "print('Resynthesis')\n",
    "IPython.display.Audio(audio_gen, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Transformer\n",
    "### __Adapted from the [Generating Piano Music with Transformer](https://colab.research.google.com/notebooks/magenta/piano_transformer/piano_transformer.ipynb) by Ian Simon, Anna Huang, Jesse Engel and Curtis \"Fjord\" Hawthorne__\n",
    "<a id=\"#music-transformer\">An</a> attention-based neural network that can generate music with improved long-term coherence. The models used here were trained on over 10,000 hours of piano recordings from YouTube, transcribed using [Onsets and Frames](https://magenta.tensorflow.org/onsets-frames) and represented using the event vocabulary from [Performance RNN](https://magenta.tensorflow.org/performance-rnn) and were compiled by the Tensorflow/Magenta team for free.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions and Helper Functions\n",
    "Define a few constants and helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SF2_PATH = 'Yamaha-C5-Salamander-JNv5.1.sf2'\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# Decode a list of IDs.\n",
    "def decode(ids, encoder):\n",
    "  ids = list(ids)\n",
    "  if text_encoder.EOS_ID in ids:\n",
    "    ids = ids[:ids.index(text_encoder.EOS_ID)]\n",
    "  return encoder.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Load Checkpoint\n",
    "Set up generation from an unconditional Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'transformer'\n",
    "hparams_set = 'transformer_tpu'\n",
    "ckpt_path = 'gs://magentadata/models/music_transformer/checkpoints/unconditional_model_16.ckpt'\n",
    "\n",
    "class PianoPerformanceLanguageModelProblem(score2perf.Score2PerfProblem):\n",
    "  @property\n",
    "  def add_eos_symbol(self):\n",
    "    return True\n",
    "\n",
    "problem = PianoPerformanceLanguageModelProblem()\n",
    "unconditional_encoders = problem.get_feature_encoders()\n",
    "\n",
    "# Set up HParams.\n",
    "hparams = trainer_lib.create_hparams(hparams_set=hparams_set)\n",
    "trainer_lib.add_problem_hparams(hparams, problem)\n",
    "hparams.num_hidden_layers = 16\n",
    "hparams.sampling_method = 'random'\n",
    "\n",
    "# Set up decoding HParams.\n",
    "decode_hparams = decoding.decode_hparams()\n",
    "decode_hparams.alpha = 0.0\n",
    "decode_hparams.beam_size = 1\n",
    "\n",
    "# Create Estimator.\n",
    "run_config = trainer_lib.create_run_config(hparams)\n",
    "estimator = trainer_lib.create_estimator(\n",
    "    model_name, hparams, run_config,\n",
    "    decode_hparams=decode_hparams)\n",
    "\n",
    "# Create input generator (so we can adjust priming and\n",
    "# decode length on the fly).\n",
    "def input_generator():\n",
    "  global targets\n",
    "  global decode_length\n",
    "  while True:\n",
    "    yield {\n",
    "        'targets': np.array([targets], dtype=np.int32),\n",
    "        'decode_length': np.array(decode_length, dtype=np.int32)\n",
    "    }\n",
    "\n",
    "# These values will be changed by subsequent cells.\n",
    "targets = []\n",
    "decode_length = 0\n",
    "\n",
    "# Start the Estimator, loading from the specified checkpoint.\n",
    "input_fn = decoding.make_input_fn_from_generator(input_generator())\n",
    "unconditional_samples = estimator.predict(\n",
    "    input_fn, checkpoint_path=ckpt_path)\n",
    "\n",
    "# \"Burn\" one.\n",
    "_ = next(unconditional_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a piano performance from scratch\n",
    "This can take a minute or so depending on the length of the performance the model ends up generating. Because we use a [representation](https://magenta.tensorflow.org/performance-rnn) where each event corresponds to a variable amount of time, the actual number of seconds generated may vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "decode_length = 1024\n",
    "\n",
    "# Generate sample events.\n",
    "sample_ids = next(unconditional_samples)['outputs']\n",
    "\n",
    "# Decode to NoteSequence.\n",
    "midi_filename = decode(\n",
    "    sample_ids,\n",
    "    encoder=unconditional_encoders['targets'])\n",
    "unconditional_ns = note_seq.midi_file_to_note_sequence(midi_filename)\n",
    "\n",
    "# Play and plot.\n",
    "note_seq.play_sequence(\n",
    "    unconditional_ns,\n",
    "    synth=note_seq.fluidsynth, sample_rate=SAMPLE_RATE, sf2_path=SF2_PATH)\n",
    "note_seq.plot_sequence(unconditional_ns)\n",
    "\n",
    "note_seq.sequence_proto_to_midi_file(\n",
    "    unconditional_ns, 'unconditional.mid') #name of generated audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Priming Sequence\n",
    "Here you can choose a priming sequence to be continued by the model.\n",
    "\n",
    "Set max_primer_seconds below to trim the primer to a fixed number of seconds (this will have no effect if the primer is already shorter than max_primer_seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = {\n",
    "    'C major arpeggio': 'c_major_arpeggio.mid',\n",
    "    'C major scale': 'c_major_scale.mid',\n",
    "    'Clair de Lune': 'clair_de_lune.mid',\n",
    "}\n",
    "primer = 'C major scale'  # choose from C-major arpeggio, C-major scale, or Clair de Lune.\n",
    "\n",
    "primer_ns = note_seq.midi_file_to_note_sequence(filenames[primer])\n",
    "\n",
    "# Handle sustain pedal in the primer.\n",
    "primer_ns = note_seq.apply_sustain_control_changes(primer_ns)\n",
    "\n",
    "# Trim to desired number of seconds.\n",
    "max_primer_seconds = 20  #@param {type:\"slider\", min:1, max:120}\n",
    "if primer_ns.total_time > max_primer_seconds:\n",
    "  print('Primer is longer than %d seconds, truncating.' % max_primer_seconds)\n",
    "  primer_ns = note_seq.extract_subsequence(\n",
    "      primer_ns, 0, max_primer_seconds)\n",
    "\n",
    "# Remove drums from primer if present.\n",
    "if any(note.is_drum for note in primer_ns.notes):\n",
    "  print('Primer contains drums; they will be removed.')\n",
    "  notes = [note for note in primer_ns.notes if not note.is_drum]\n",
    "  del primer_ns.notes[:]\n",
    "  primer_ns.notes.extend(notes)\n",
    "\n",
    "# Set primer instrument and program.\n",
    "for note in primer_ns.notes:\n",
    "  note.instrument = 1\n",
    "  note.program = 0\n",
    "\n",
    "# Play and plot the primer.\n",
    "note_seq.play_sequence(\n",
    "    primer_ns,\n",
    "    synth=note_seq.fluidsynth, sample_rate=SAMPLE_RATE, sf2_path=SF2_PATH)\n",
    "note_seq.plot_sequence(primer_ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Continuation\n",
    "Continue a piano performance, starting with the chosen priming sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = unconditional_encoders['targets'].encode_note_sequence(\n",
    "    primer_ns)\n",
    "\n",
    "# Remove the end token from the encoded primer.\n",
    "targets = targets[:-1]\n",
    "\n",
    "decode_length = max(0, 4096 - len(targets))\n",
    "if len(targets) >= 4096:\n",
    "  print('Primer has more events than maximum sequence length; nothing will be generated.')\n",
    "\n",
    "# Generate sample events.\n",
    "sample_ids = next(unconditional_samples)['outputs']\n",
    "\n",
    "# Decode to NoteSequence.\n",
    "midi_filename = decode(\n",
    "    sample_ids,\n",
    "    encoder=unconditional_encoders['targets'])\n",
    "ns = note_seq.midi_file_to_note_sequence(midi_filename)\n",
    "\n",
    "# Append continuation to primer.\n",
    "continuation_ns = note_seq.concatenate_sequences([primer_ns, ns])\n",
    "\n",
    "# Play and plot.\n",
    "note_seq.play_sequence(\n",
    "    continuation_ns,\n",
    "    synth=note_seq.fluidsynth, sample_rate=SAMPLE_RATE, sf2_path=SF2_PATH)\n",
    "note_seq.plot_sequence(continuation_ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save generated audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_seq.sequence_proto_to_midi_file(\n",
    "    continuation_ns, 'continuation.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melody-Conditioned Piano Performance Model\n",
    "<a id=\"#melody-conditioned-piano-transformer\">Set</a> up generation from a melody-conditioned Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'transformer'\n",
    "hparams_set = 'transformer_tpu'\n",
    "ckpt_path = 'gs://magentadata/models/music_transformer/checkpoints/melody_conditioned_model_16.ckpt'\n",
    "\n",
    "class MelodyToPianoPerformanceProblem(score2perf.AbsoluteMelody2PerfProblem):\n",
    "  @property\n",
    "  def add_eos_symbol(self):\n",
    "    return True\n",
    "\n",
    "problem = MelodyToPianoPerformanceProblem()\n",
    "melody_conditioned_encoders = problem.get_feature_encoders()\n",
    "\n",
    "# Set up HParams.\n",
    "hparams = trainer_lib.create_hparams(hparams_set=hparams_set)\n",
    "trainer_lib.add_problem_hparams(hparams, problem)\n",
    "hparams.num_hidden_layers = 16\n",
    "hparams.sampling_method = 'random'\n",
    "\n",
    "# Set up decoding HParams.\n",
    "decode_hparams = decoding.decode_hparams()\n",
    "decode_hparams.alpha = 0.0\n",
    "decode_hparams.beam_size = 1\n",
    "\n",
    "# Create Estimator.\n",
    "run_config = trainer_lib.create_run_config(hparams)\n",
    "estimator = trainer_lib.create_estimator(\n",
    "    model_name, hparams, run_config,\n",
    "    decode_hparams=decode_hparams)\n",
    "\n",
    "# These values will be changed by the following cell.\n",
    "inputs = []\n",
    "decode_length = 0\n",
    "\n",
    "# Create input generator.\n",
    "def input_generator():\n",
    "  global inputs\n",
    "  while True:\n",
    "    yield {\n",
    "        'inputs': np.array([[inputs]], dtype=np.int32),\n",
    "        'targets': np.zeros([1, 0], dtype=np.int32),\n",
    "        'decode_length': np.array(decode_length, dtype=np.int32)\n",
    "    }\n",
    "\n",
    "# Start the Estimator, loading from the specified checkpoint.\n",
    "input_fn = decoding.make_input_fn_from_generator(input_generator())\n",
    "melody_conditioned_samples = estimator.predict(\n",
    "    input_fn, checkpoint_path=ckpt_path)\n",
    "\n",
    "# \"Burn\" one.\n",
    "_ = next(melody_conditioned_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Melody\n",
    "Here you can choose a melody to be accompanied by the model. There are a few preassigned options to choose from: \"Twinkle Twinkle Little Star\", \"Mary Had a Little Lamb\" and \"Row Row Row your Boat\" You can use your own melody too. If your MIDI file is polyphonic, the notes with the highest pitch will be used as the melody."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens to insert between melody events.\n",
    "event_padding = 2 * [note_seq.MELODY_NO_EVENT]\n",
    "\n",
    "melodies = {\n",
    "    'Mary Had a Little Lamb': [\n",
    "        64, 62, 60, 62, 64, 64, 64, note_seq.MELODY_NO_EVENT,\n",
    "        62, 62, 62, note_seq.MELODY_NO_EVENT,\n",
    "        64, 67, 67, note_seq.MELODY_NO_EVENT,\n",
    "        64, 62, 60, 62, 64, 64, 64, 64,\n",
    "        62, 62, 64, 62, 60, note_seq.MELODY_NO_EVENT,\n",
    "        note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT\n",
    "    ],\n",
    "    'Row Row Row Your Boat': [\n",
    "        60, note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT,\n",
    "        60, note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT,\n",
    "        60, note_seq.MELODY_NO_EVENT, 62,\n",
    "        64, note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT,\n",
    "        64, note_seq.MELODY_NO_EVENT, 62,\n",
    "        64, note_seq.MELODY_NO_EVENT, 65,\n",
    "        67, note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT,\n",
    "        note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT,\n",
    "        72, 72, 72, 67, 67, 67, 64, 64, 64, 60, 60, 60,\n",
    "        67, note_seq.MELODY_NO_EVENT, 65,\n",
    "        64, note_seq.MELODY_NO_EVENT, 62,\n",
    "        60, note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT,\n",
    "        note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT, note_seq.MELODY_NO_EVENT\n",
    "    ],\n",
    "    'Twinkle Twinkle Little Star': [\n",
    "        60, 60, 67, 67, 69, 69, 67, note_seq.MELODY_NO_EVENT,\n",
    "        65, 65, 64, 64, 62, 62, 60, note_seq.MELODY_NO_EVENT,\n",
    "        67, 67, 65, 65, 64, 64, 62, note_seq.MELODY_NO_EVENT,\n",
    "        67, 67, 65, 65, 64, 64, 62, note_seq.MELODY_NO_EVENT,\n",
    "        60, 60, 67, 67, 69, 69, 67, note_seq.MELODY_NO_EVENT,\n",
    "        65, 65, 64, 64, 62, 62, 60, note_seq.MELODY_NO_EVENT        \n",
    "    ]\n",
    "}\n",
    "\n",
    "melody = 'Twinkle Twinkle Little Star'  #@param ['Mary Had a Little Lamb', 'Row Row Row Your Boat', 'Twinkle Twinkle Little Star', 'Upload your own!']\n",
    "\n",
    "if melody == 'Upload your own!':\n",
    "  # Extract melody from user-uploaded MIDI file.\n",
    "  melody_ns = upload_midi()\n",
    "  melody_instrument = note_seq.infer_melody_for_sequence(melody_ns)\n",
    "  notes = [note for note in melody_ns.notes\n",
    "           if note.instrument == melody_instrument]\n",
    "  del melody_ns.notes[:]\n",
    "  melody_ns.notes.extend(\n",
    "      sorted(notes, key=lambda note: note.start_time))\n",
    "  for i in range(len(melody_ns.notes) - 1):\n",
    "    melody_ns.notes[i].end_time = melody_ns.notes[i + 1].start_time\n",
    "  inputs = melody_conditioned_encoders['inputs'].encode_note_sequence(\n",
    "      melody_ns)\n",
    "else:\n",
    "  # Use one of the provided melodies.\n",
    "  events = [event + 12 if event != note_seq.MELODY_NO_EVENT else event\n",
    "            for e in melodies[melody]\n",
    "            for event in [e] + event_padding]\n",
    "  inputs = melody_conditioned_encoders['inputs'].encode(\n",
    "      ' '.join(str(e) for e in events))\n",
    "  melody_ns = note_seq.Melody(events).to_sequence(qpm=150)\n",
    "\n",
    "# Play and plot the melody.\n",
    "note_seq.play_sequence(\n",
    "    melody_ns,\n",
    "    synth=note_seq.fluidsynth, sample_rate=SAMPLE_RATE, sf2_path=SF2_PATH)\n",
    "note_seq.plot_sequence(melody_ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go crazy. Generate Accompaniment for Melody. \n",
    "Generate a piano performance consisting of the chosen melody plus accompaniment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample events.\n",
    "decode_length = 4096\n",
    "sample_ids = next(melody_conditioned_samples)['outputs']\n",
    "\n",
    "# Decode to NoteSequence.\n",
    "midi_filename = decode(\n",
    "    sample_ids,\n",
    "    encoder=melody_conditioned_encoders['targets'])\n",
    "accompaniment_ns = note_seq.midi_file_to_note_sequence(midi_filename)\n",
    "\n",
    "# Play and plot.\n",
    "note_seq.play_sequence(\n",
    "    accompaniment_ns,\n",
    "    synth=note_seq.fluidsynth, sample_rate=SAMPLE_RATE, sf2_path=SF2_PATH)\n",
    "note_seq.plot_sequence(accompaniment_ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_seq.sequence_proto_to_midi_file(\n",
    "    accompaniment_ns, 'accompaniment.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLyrics\n",
    "## Using the power of Jukebox, an open-source Deep Learning music generation library, developed by OpenAI to generate music from nothing but lyrics.\n",
    "### Adapted from the Jukebox Colab Notebook\n",
    "\n",
    "Note: <a href=\"deeplyrics\">We</a> highly recommend that you follow the recommended hardware specifications specified earlier for this particular segment. We also highly recommend that you run this on a GPU with atleast 16GB of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from 1B or 5B model\n",
    "The 5B model is more robust and will provide better results compared to the 1B model. However, the 1B model is signifantly faster and less resource intensive. Use it if you have less than 16GB of GPU memory on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"1b_lyrics\" # Change this to \"5b_lyrics\" if you choose to use the 5B model\n",
    "hps = Hyperparams() #load hyperparams\n",
    "hps.sr = 44100 #sample rate\n",
    "hps.n_samples = 3 if model=='5b_lyrics' else 3 #number of samples to generate\n",
    "hps.name = 'samples'\n",
    "chunk_size = 16 if model==\"5b_lyrics\" else 32\n",
    "max_batch_size = 3 if model==\"5b_lyrics\" else 16\n",
    "hps.levels = 3\n",
    "hps.hop_fraction = [.5,.5,.125]\n",
    "\n",
    "vqvae, *priors = MODELS[model]\n",
    "vqvae = make_vqvae(setup_hparams(vqvae, dict(sample_length = 1048576)), device)\n",
    "top_prior = make_prior(setup_hparams(priors[-1], dict()), vqvae, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify your choice of artist, genre, lyrics, and length of musical sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_length_in_seconds = 60          # Full length of musical sample to generate - we find songs in the 1 to 4 minute\n",
    "                                       # range work well, with generation time proportional to sample length.  \n",
    "                                       # This total length affects how quickly the model \n",
    "                                       # progresses through lyrics (model also generates differently\n",
    "                                       # depending on if it thinks it's in the beginning, middle, or end of sample)\n",
    "\n",
    "hps.sample_length = (int(sample_length_in_seconds*hps.sr)//top_prior.raw_to_tokens)*top_prior.raw_to_tokens\n",
    "assert hps.sample_length >= top_prior.n_ctx*top_prior.raw_to_tokens, f'Please choose a larger sampling rate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We chose to work with Eminem's voice with lyrics from \"Paid my Dues\" by NF\n",
    "\n",
    "metas = [dict(artist = \"Eminem\",\n",
    "            genre = \"Hip Hop\",\n",
    "            total_length = hps.sample_length,\n",
    "            offset = 0,\n",
    "            lyrics = \"\"\"II spit it with ease, so leave it to me\n",
    "            You doubt it but you better believe\n",
    "            I'm on a rampage hit 'em with the record release\n",
    "            Dependin' the week, I'm prolly gonna have to achieve another goal\n",
    "            Let me go when I'm over the beat\n",
    "            I go into beast mode like I'm ready to feast\n",
    "            I'm fed up with these thieves tryna get me to bleed\n",
    "            They wanna see me take an L? (yup, see what I mean)\n",
    "            How many records I gotta give you to get with the program?\n",
    "            Taken for granted I'm 'bout to give you the whole plan\n",
    "            Open your mind up and take a look at the blueprint\n",
    "            Debate if you gotta, but gotta hold it with both hands\n",
    "            To pick up the bars you gotta be smart\n",
    "            You really gotta dig in your heart if you wanna get to the root of an issue\n",
    "            Pursuin' the mental can be dark and be difficult\n",
    "            But the payoff at the end of it, can help you to get through it, hey\n",
    "            \"\"\",\n",
    "            ),\n",
    "          ] * hps.n_samples\n",
    "labels = [None, None, top_prior.labeller.get_batch_labels(metas, 'cuda')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally adjust the sampling temperature (set it around 1 for the best results).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_temperature = .98\n",
    "\n",
    "lower_batch_size = 16\n",
    "max_batch_size = 3 if model == \"5b_lyrics\" else 16\n",
    "lower_level_chunk_size = 32\n",
    "chunk_size = 16 if model == \"5b_lyrics\" else 32\n",
    "sampling_kwargs = [dict(temp=.99, fp16=True, max_batch_size=lower_batch_size,\n",
    "                        chunk_size=lower_level_chunk_size),\n",
    "                    dict(temp=0.99, fp16=True, max_batch_size=lower_batch_size,\n",
    "                         chunk_size=lower_level_chunk_size),\n",
    "                    dict(temp=sampling_temperature, fp16=True, \n",
    "                         max_batch_size=max_batch_size, chunk_size=chunk_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to sample from the model. We'll generate the top level (2) first, followed by the first upsampling (level 1), and the second upsampling (0).  If you are using a local machine, you can also load all models directly with make_models, and then use sample.py's ancestral_sampling to put this all in one step.\n",
    "\n",
    "After each level, we decode to raw audio and save the audio files.   \n",
    "\n",
    "This next cell will take a while (approximately 10 minutes per 20 seconds of music sample), similar to synthesizing audio as we demonstrated earlier.\n",
    "\n",
    "Approximate time required for a 60 second song (~1,000,000 interpolations): 30 mins-1 hour on a GeForce RTX GPU and 5-10 minutes on a Quadro or Tesla GPU. These audio files are compressed and will be of lower audio quality than the original. You may find these at {hps.name}/level_2/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = [t.zeros(hps.n_samples,0,dtype=t.long, device='cuda') for _ in range(len(priors))]\n",
    "zs = _sample(zs, labels, sampling_kwargs, [None, None, top_prior], [2], hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(f'{hps.name}/level_2/item_0.wav') #if you want to hear it directly from Jupyter or Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling\n",
    "\n",
    "The following code block will allow you to upsample your previously generated audio using Neural Networks. This process is GPU dependant and will take a long time to complete if you do not meet the recommended hardware requirements. With a GPU, this should take about 4 minutes per 1 second of audio per a batch. Approximate time required for a 60 second song (~800,000 interpolations): 8-12 hours on a GeForce GPU and 2-8 hours on a Quadro or Tesla GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this False if you are on a local machine that has enough memory (this allows you to do the\n",
    "# lyrics alignment visualization during the upsampling stage). For a hosted runtime, \n",
    "# we'll need to go ahead and delete the top_prior if you are using the 5b_lyrics model.\n",
    "if True:\n",
    "  del top_prior\n",
    "  empty_cache()\n",
    "  top_prior=None\n",
    "upsamplers = [make_prior(setup_hparams(prior, dict()), vqvae, 'cpu') for prior in priors[:-1]]\n",
    "labels[:2] = [prior.labeller.get_batch_labels(metas, 'cuda') for prior in upsamplers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = upsample(zs, labels, sampling_kwargs, [*upsamplers, top_prior], hps) #Note: This is the code that upsamples the previously \n",
    "                                                                          #generated low-quality audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(f'{hps.name}/level_0/item_0.wav') #if you want to hear it directly from Jupyter or Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del upsamplers\n",
    "empty_cache() #clears stored cache from all the processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alright Folks, that's it! More to come soon."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "511b9326a14b8760749658bd39443b1e3bf21c23cddb64ff2a1627315b974833"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('jukebox': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
