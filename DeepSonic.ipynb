{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Sonic\n",
    "### ___Chris Pagolu, Joshua JJ Wonder___\n",
    "\n",
    "[DeepSonic](https://magenta.tensorflow.org/) is a fully open source Deep Learning music experiment that is capable of synthesizing, generating, remixing, modifying music, all using the power of AI. Thanks to powerful open-source libraries such as Magenta by Tensorflow/Google and Jukebox by OpenAI, we were able to create a multi-functional AI Audio Engineer. \n",
    "\n",
    "Note: This notebook runs all code natively. No cloud service is required unless you do not have a dedicated Nvidia GPU."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Basic Hardware Requirements and Recommendations\n",
    "\n",
    "The DeepSonic Experiment requires considerably powerful hardware. \n",
    "\n",
    "An NVIDIA Geforce RTX 2000 (Turing) Series GPU with atleast 8GB of VRAM is required, at the least. A cloud-based NVIDIA Tesla or a server NVIDIA Quadro GPU with atleast 16GB VRAM is recommended, while a supercomputer will perform best, depending on the task. There are no explicit CPU requirements for DeepSonic, however, an AMD Ryzen 3 3200 or Intel Core i3 8100 (or higher) is recommended. The more powerful, the better."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# How to get started?\n",
    "\n",
    "To get started with DeepSonic, all you have to do is install Magenta and Jukebox from their official GitHub repositories.\n",
    "\n",
    "# Quick Install Guide:\n",
    "\n",
    "Run the follow code in your shell (taken from the official Magenta and Jukebox wiki) to install the required tools. We recommend using a Debian-based operating system. The Windows Subsystem for Linux (WSL2) and Windows didn't appear to work at the time of our testing, possibly due to early support for Cuda on WSL. Also note: root privileges are required for installing audio libraries.\n",
    "\n",
    "```bash\n",
    "# Required commands:\n",
    "\n",
    "sudo apt-get update && sudo apt-get install build-essential libasound2-dev libjack-dev portaudio19-dev\n",
    "curl https://raw.githubusercontent.com/tensorflow/magenta/main/magenta/tools/magenta-install.sh > /tmp/magenta-install.sh\n",
    "bash /tmp/magenta-install.sh\n",
    "conda create --name jukebox python=3.7.5\n",
    "conda activate jukebox\n",
    "conda install mpi4py=3.0.3 # if this fails, try: pip install mpi4py==3.0.3\n",
    "conda install pytorch=1.4 torchvision=0.5 cudatoolkit=10.0 -c pytorch\n",
    "git clone https://github.com/openai/jukebox.git\n",
    "cd jukebox\n",
    "pip install -r requirements.txt\n",
    "pip install -e .\n",
    "conda install av=7.0.01 -c conda-forge \n",
    "pip install ./tensorboardX\n",
    " \n",
    "# Following two commands are optional: Apex for faster training with fused_adam \n",
    "\n",
    "conda install pytorch=1.1 torchvision=0.3 cudatoolkit=10.0 -c pytorch\n",
    "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "print('Importing Modules...\\n')\n",
    "#basic libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "%matplotlib inline\n",
    "\n",
    "#magenta libraries\n",
    "from magenta.models.nsynth import utils\n",
    "from magenta.models.nsynth.wavenet import fastgen\n",
    "from note_seq.notebook_utils import colab_play as play\n",
    "\n",
    "\n",
    "#jukebox libraries\n",
    "\n",
    "import jukebox\n",
    "import torch as t\n",
    "import librosa\n",
    "from jukebox.make_models import make_vqvae, make_prior, MODELS, make_model\n",
    "from jukebox.hparams import Hyperparams, setup_hparams\n",
    "from jukebox.sample import sample_single_window, _sample, \\\n",
    "                           sample_partial_window, upsample\n",
    "from jukebox.utils.dist_utils import setup_dist_from_mpi\n",
    "from jukebox.utils.torch_utils import empty_cache\n",
    "rank, local_rank, device = setup_dist_from_mpi()\n",
    "\n",
    "get_name = lambda f: os.path.splitext(os.path.basename(f))[0]\n",
    "\n",
    "print('Sucess!! Environment is now setup.')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Importing Modules...\n",
      "\n",
      "WARNING:tensorflow:From /home/chris/miniconda3/envs/jukebox/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:549: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "Using cuda True\n",
      "Sucess!! Environment is now setup.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "!nvidia-smi #checks if cuda and nvidia drivers are working properly"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sun Aug 22 15:07:26 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:09:00.0  On |                  N/A |\n",
      "| 25%   49C    P5    16W / 215W |   2234MiB /  7981MiB |      6%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1800      G   /usr/lib/xorg/Xorg               1217MiB |\n",
      "|    0   N/A  N/A      1933      G   /usr/bin/gnome-shell              199MiB |\n",
      "|    0   N/A  N/A      2524      G   ...AAAAAAAAA= --shared-files      233MiB |\n",
      "|    0   N/A  N/A      2659      G   ...AAAAAAAAA= --shared-files      201MiB |\n",
      "|    0   N/A  N/A     13622      C   ...3/envs/magenta/bin/python       97MiB |\n",
      "|    0   N/A  N/A     17164      G   telegram-desktop                  268MiB |\n",
      "|    0   N/A  N/A     82016      G   /usr/bin/totem                     10MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DeepSynth\n",
    "### __Adapted from the [EZSynth Experiment](https://colab.research.google.com/notebooks/magenta/nsynth/nsynth.ipynb) by Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Douglas Eck, Karen Simonyan, Mohammad Norouzi__\n",
    "Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders\n",
    "\n",
    "### Additional Resources (as provided in the [EZSynth Notebook](https://colab.research.google.com/notebooks/magenta/nsynth/nsynth.ipynb)):\n",
    "* [Nat and Friends \"Behind the scenes\"](https://www.youtube.com/watch?v=BOoSy-Pg8is)\n",
    "* [Original Blog Post](https://magenta.tensorflow.org/nsynth)\n",
    "* [NSynth Instrument](https://magenta.tensorflow.org/nsynth-instrument)\n",
    "* [Jupyter Notebook Tutorial](https://magenta.tensorflow.org/nsynth-fastgen)\n",
    "* [ArXiv Paper](https://arxiv.org/abs/1704.01279)\n",
    "* [Github Code](https://github.com/tensorflow/magenta/tree/main/magenta/models/nsynth)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are two pretrained models to choose from (thanks, Google :)): one trained on the individual instrument notes of the [NSynth Dataset](https://magenta.tensorflow.org/datasets/nsynth) (\"Instruments\"), and another trained on a variety of voices in the wild for an art project (\"Voices\", mixture of singing and speaking). The Instruments model was trained on a larger quantity of data, so tends to generalize a bit better. Neither reconstructs audio perfectly, but both add their own unique character to sounds. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Choose a Model { vertical-output: true, run: \"auto\" }\n",
    "Model = \"Instruments\" #@param [\"Instruments\", \"Voices\"] {type:\"string\"}\n",
    "ckpts = {'Instruments': 'wavenet-ckpt/model.ckpt-200000',\n",
    "         'Voices': 'wavenet-voice-ckpt/model.ckpt-200000'}\n",
    "\n",
    "ckpt_path = ckpts[Model]\n",
    "print('Using model pretrained on %s.' % Model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use local audio files\n",
    "\n",
    "In the next section, you may choose to specify which audio file you want to use for the audio synthesis. Note: the larger your audio file, the longer it'll take to encode and the longer it'll take to synthesize the audio, depending on how powerful your GPU is."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Set Sound Length (in Seconds) { vertical-output: true, run: \"auto\" }\n",
    "Length = 60.0 #set the length of your synthesized audio\n",
    "SR = 16000\n",
    "SAMPLE_LENGTH = int(SR * Length) #audio length"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Upload sound files (.wav, .mp3)\n",
    "\n",
    "try:\n",
    "  file_list, audio_list = [], [] #creates numpy arrays for file and audio lists\n",
    "  fname=\"test.mp3\" # name of the audio file\n",
    "  audio = utils.load_audio(fname, sample_length=SAMPLE_LENGTH, sr=SR)  #loads audio file for magenta to process\n",
    "  file_list.append(fname)\n",
    "  audio_list.append(audio)\n",
    "  names = [get_name(f) for f in file_list]\n",
    "  # Pad and peak normalize\n",
    "  for i in range(len(audio_list)):\n",
    "    audio_list[i] = audio_list[i] / np.abs(audio_list[i]).max()\n",
    "\n",
    "    if len(audio_list[i]) < SAMPLE_LENGTH:\n",
    "      padding = SAMPLE_LENGTH - len(audio_list[i])\n",
    "      audio_list[i] = np.pad(audio_list[i], (0, padding), 'constant')\n",
    "\n",
    "  audio_list = np.array(audio_list)\n",
    "except Exception as e:\n",
    "  print(\"Error encountered. Sure the file is .wav or .mp3? Does your GPU have enough memory left?\")\n",
    "  print(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The below code may take some time, depending on your GPU."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Generate Encodings\n",
    "audio = np.array(audio_list)\n",
    "z = fastgen.encode(audio, ckpt_path, SAMPLE_LENGTH)\n",
    "print('Encoded %d files' % z.shape[0])\n",
    "\n",
    "\n",
    "# Start with reconstructions\n",
    "z_list = [z_ for z_ in z]\n",
    "name_list = ['recon_' + name_ for name_ in names]\n",
    "\n",
    "# Add all the mean interpolations\n",
    "n = len(names)\n",
    "for i in range(n - 1):\n",
    "  for j in range(i + 1, n):\n",
    "    new_z = (z[i] + z[j]) / 2.0\n",
    "    new_name = 'interp_' + names[i] + '_X_'+ names[j]\n",
    "    z_list.append(new_z)\n",
    "    name_list.append(new_name)\n",
    "\n",
    "print(\"%d total: %d reconstructions and %d interpolations\" % (len(name_list), n, len(name_list) - n))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final Step: Synthesize\n",
    "\n",
    "With a GPU, this should take about 4 minutes per 1 second of audio per a batch. Approximate time required for a 60 second song (~1,000,000 interpolations): 8-12 hours on a GeForce GPU and 2-8 hours on a Quadro or Tesla GPU. After that, your synthesized audio will appear in the same directory as this notebook (can be found as `recon_<name of audio>.mp3` or `recon_<name of audio>.wav`). )"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Synthesize Interpolations\n",
    "print('Total Iterations to Complete: %d\\n' % SAMPLE_LENGTH)\n",
    "\n",
    "encodings = np.array(z_list)\n",
    "save_paths = [name + '.wav' for name in name_list]\n",
    "fastgen.synthesize(encodings,\n",
    "                   save_paths=save_paths,\n",
    "                   checkpoint_path=ckpt_path,\n",
    "                   samples_per_save=int(SAMPLE_LENGTH / 10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DeepLyrics\n",
    "## Using the power of Jukebox, an open-source Deep Learning music generation library, developed by OpenAI to generate music from nothing but lyrics.\n",
    "### Adapted from the Jukebox Colab Notebook\n",
    "\n",
    "Note: We highly recommend that you follow the recommended hardware specifications specified earlier for this particular segment. We also highly recommend that you run this on a GPU with atleast 16GB of memory."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample from 1B or 5B model\n",
    "The 5B model is more robust and will provide better results compared to the 1B model. However, the 1B model is signifantly faster and less resource intensive. Use it if you have less than 16GB of GPU memory on your system."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = \"1b_lyrics\" # Change this to \"5b_lyrics\" if you choose to use the 5B model\n",
    "hps = Hyperparams() #load hyperparams\n",
    "hps.sr = 44100 #sample rate\n",
    "hps.n_samples = 3 if model=='5b_lyrics' else 3 #number of samples to generate\n",
    "hps.name = 'samples'\n",
    "chunk_size = 16 if model==\"5b_lyrics\" else 32\n",
    "max_batch_size = 3 if model==\"5b_lyrics\" else 16\n",
    "hps.levels = 3\n",
    "hps.hop_fraction = [.5,.5,.125]\n",
    "\n",
    "vqvae, *priors = MODELS[model]\n",
    "vqvae = make_vqvae(setup_hparams(vqvae, dict(sample_length = 1048576)), device)\n",
    "top_prior = make_prior(setup_hparams(priors[-1], dict()), vqvae, device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Specify your choice of artist, genre, lyrics, and length of musical sample. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_length_in_seconds = 60          # Full length of musical sample to generate - we find songs in the 1 to 4 minute\n",
    "                                       # range work well, with generation time proportional to sample length.  \n",
    "                                       # This total length affects how quickly the model \n",
    "                                       # progresses through lyrics (model also generates differently\n",
    "                                       # depending on if it thinks it's in the beginning, middle, or end of sample)\n",
    "\n",
    "hps.sample_length = (int(sample_length_in_seconds*hps.sr)//top_prior.raw_to_tokens)*top_prior.raw_to_tokens\n",
    "assert hps.sample_length >= top_prior.n_ctx*top_prior.raw_to_tokens, f'Please choose a larger sampling rate'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#We chose to work with Eminem's voice with lyrics from \"Paid my Dues\" by NF\n",
    "\n",
    "metas = [dict(artist = \"Eminem\",\n",
    "            genre = \"Hip Hop\",\n",
    "            total_length = hps.sample_length,\n",
    "            offset = 0,\n",
    "            lyrics = \"\"\"II spit it with ease, so leave it to me\n",
    "            You doubt it but you better believe\n",
    "            I'm on a rampage hit 'em with the record release\n",
    "            Dependin' the week, I'm prolly gonna have to achieve another goal\n",
    "            Let me go when I'm over the beat\n",
    "            I go into beast mode like I'm ready to feast\n",
    "            I'm fed up with these thieves tryna get me to bleed\n",
    "            They wanna see me take an L? (yup, see what I mean)\n",
    "            How many records I gotta give you to get with the program?\n",
    "            Taken for granted I'm 'bout to give you the whole plan\n",
    "            Open your mind up and take a look at the blueprint\n",
    "            Debate if you gotta, but gotta hold it with both hands\n",
    "            To pick up the bars you gotta be smart\n",
    "            You really gotta dig in your heart if you wanna get to the root of an issue\n",
    "            Pursuin' the mental can be dark and be difficult\n",
    "            But the payoff at the end of it, can help you to get through it, hey\n",
    "            \"\"\",\n",
    "            ),\n",
    "          ] * hps.n_samples\n",
    "labels = [None, None, top_prior.labeller.get_batch_labels(metas, 'cuda')]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optionally adjust the sampling temperature (set it around 1 for the best results).  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sampling_temperature = .98\n",
    "\n",
    "lower_batch_size = 16\n",
    "max_batch_size = 3 if model == \"5b_lyrics\" else 16\n",
    "lower_level_chunk_size = 32\n",
    "chunk_size = 16 if model == \"5b_lyrics\" else 32\n",
    "sampling_kwargs = [dict(temp=.99, fp16=True, max_batch_size=lower_batch_size,\n",
    "                        chunk_size=lower_level_chunk_size),\n",
    "                    dict(temp=0.99, fp16=True, max_batch_size=lower_batch_size,\n",
    "                         chunk_size=lower_level_chunk_size),\n",
    "                    dict(temp=sampling_temperature, fp16=True, \n",
    "                         max_batch_size=max_batch_size, chunk_size=chunk_size)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we're ready to sample from the model. We'll generate the top level (2) first, followed by the first upsampling (level 1), and the second upsampling (0).  If you are using a local machine, you can also load all models directly with make_models, and then use sample.py's ancestral_sampling to put this all in one step.\n",
    "\n",
    "After each level, we decode to raw audio and save the audio files.   \n",
    "\n",
    "This next cell will take a while (approximately 10 minutes per 20 seconds of music sample), similar to synthesizing audio as we demonstrated earlier.\n",
    "\n",
    "Approximate time required for a 60 second song (~1,000,000 interpolations): 30 mins-1 hour on a GeForce RTX GPU and 5-10 minutes on a Quadro or Tesla GPU. These audio files are compressed and will be of lower audio quality than the original. You may find these at {hps.name}/level_2/."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "zs = [t.zeros(hps.n_samples,0,dtype=t.long, device='cuda') for _ in range(len(priors))]\n",
    "zs = _sample(zs, labels, sampling_kwargs, [None, None, top_prior], [2], hps)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Audio(f'{hps.name}/level_2/item_0.wav') #if you want to hear it directly from Jupyter or Colab"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Upsampling\n",
    "\n",
    "The following code block will allow you to upsample your previously generated audio using Neural Networks. This process is GPU dependant and will take a long time to complete if you do not meet the recommended hardware requirements. With a GPU, this should take about 4 minutes per 1 second of audio per a batch. Approximate time required for a 60 second song (~800,000 interpolations): 8-12 hours on a GeForce GPU and 2-8 hours on a Quadro or Tesla GPU. The resultant file will be available at "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set this False if you are on a local machine that has enough memory (this allows you to do the\n",
    "# lyrics alignment visualization during the upsampling stage). For a hosted runtime, \n",
    "# we'll need to go ahead and delete the top_prior if you are using the 5b_lyrics model.\n",
    "if True:\n",
    "  del top_prior\n",
    "  empty_cache()\n",
    "  top_prior=None\n",
    "upsamplers = [make_prior(setup_hparams(prior, dict()), vqvae, 'cpu') for prior in priors[:-1]]\n",
    "labels[:2] = [prior.labeller.get_batch_labels(metas, 'cuda') for prior in upsamplers]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "zs = upsample(zs, labels, sampling_kwargs, [*upsamplers, top_prior], hps) #Note: This is the code that upsamples the previously \n",
    "                                                                          #generated low-quality audio file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Audio(f'{hps.name}/level_0/item_0.wav') #if you want to hear it directly from Jupyter or Colab"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "del upsamplers\n",
    "empty_cache() #clears stored cache from all the processing"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6b02fdeae505>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.5 64-bit"
  },
  "interpreter": {
   "hash": "3a11d3723f830b6621e9fd4cc15ea8ceca39a4ebcc01163e03ac92a8e9df01db"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}